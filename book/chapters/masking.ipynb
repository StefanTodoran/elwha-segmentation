{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjfwt2MCzQuu"
      },
      "source": [
        "# River Boundary Masking\n",
        "\n",
        "## Introduction\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6MqhHWcHW3sG"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch transformers datasets tqdm monai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uMd2zYm_W3sG"
      },
      "outputs": [],
      "source": [
        "!pip install -q huggingface_hub\n",
        "fetch_updated = True\n",
        "\n",
        "if fetch_updated:\n",
        "    !huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "fetch_updated = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMIPy6lNzQuw",
        "outputId": "790cb2ac-cc02-4d85-ebdc-7da3d336ef02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetchng data from huggingface server...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aaf808b322814d8baef7c43dd03bc96e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/434 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d541f72fd984a00b5056b941e7b7255",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/200M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab1e0cfefa6b496585aaedc36e4ad9b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/23.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6ca563d82b8458fa5de117e47ba496a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/198 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad2929f8c9e540b797edba76d2c19bde",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/22 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e54193adc4ac4753a350bd4d1df3fd12",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/198 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8da5cdb066c2405f9c16aee043be59a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/22 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['image', 'label'],\n",
            "    num_rows: 198\n",
            "})\n",
            "Dataset({\n",
            "    features: ['image', 'label'],\n",
            "    num_rows: 22\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, load_from_disk, Dataset\n",
        "\n",
        "training_data_path = \"./elwha-dataset-tiny-train.hf\"\n",
        "validation_data_path = \"./elwha-dataset-tiny-val.hf\"\n",
        "\n",
        "if fetch_updated:\n",
        "    print(\"Fetchng data from huggingface server...\")\n",
        "    training_data: Dataset = load_dataset(\"stodoran/elwha-segmentation-tiny\", split=\"train\")  # type: ignore\n",
        "    validation_data: Dataset = load_dataset(\"stodoran/elwha-segmentation-tiny\", split=\"validation\")  # type: ignore\n",
        "    training_data.save_to_disk(training_data_path)\n",
        "    validation_data.save_to_disk(validation_data_path)\n",
        "else:\n",
        "    print(\"Loading data from disk...\")\n",
        "    training_data = load_from_disk(training_data_path)  # type: ignore\n",
        "    validation_data = load_from_disk(validation_data_path)  # type: ignore\n",
        "\n",
        "print(training_data)\n",
        "print(validation_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "u5V5Q32RzQux"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Circle\n",
        "\n",
        "def overlay_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "def show_control_points(coords, labels, ax, min_distance=None, min_dist_negative=None, marker_size=375):\n",
        "    if len(coords) == 0 or len(labels) == 0: return\n",
        "    pos_points = coords[labels == 1]\n",
        "    neg_points = coords[labels == 0]\n",
        "\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "\n",
        "    if min_distance is not None:\n",
        "        for point in pos_points:\n",
        "            circle = Circle((point[0], point[1]), min_distance, color=\"white\", fill=False, linestyle=\"dashed\")\n",
        "            ax.add_patch(circle)\n",
        "\n",
        "        for point in neg_points:\n",
        "            _neg_distance = min_dist_negative or min_distance\n",
        "            circle = Circle((point[0], point[1]), _neg_distance, color=\"white\", fill=False, linestyle=\"dashed\")\n",
        "            ax.add_patch(circle)\n",
        "\n",
        "        ax.set_xlim(xlim)\n",
        "        ax.set_ylim(ylim)\n",
        "\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color=\"green\", marker=\"P\", s=marker_size, edgecolor=\"white\", linewidth=1.25)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color=\"red\", marker=\"X\", s=marker_size, edgecolor=\"white\", linewidth=1.25)\n",
        "\n",
        "def show_bounding_box(box, ax):\n",
        "    lw = 2\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0] - lw, box[3] - box[1] - lw\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=lw))  # type: ignore\n",
        "\n",
        "def preview_datapoint(datapoint, points=None, bbox=None, mask=True, min_distance=None, min_dist_negative=None):\n",
        "    fig, axes = plt.subplots()\n",
        "\n",
        "    axes.imshow(datapoint[\"image\"])\n",
        "    if mask: overlay_mask(np.array(datapoint[\"label\"]), axes)\n",
        "    if points: show_control_points(*points, axes, min_distance, min_dist_negative)  # type: ignore\n",
        "    if bbox: show_bounding_box(bbox, axes)\n",
        "\n",
        "    axes.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "CEzgXAgzmVaU"
      },
      "outputs": [],
      "source": [
        "from skimage.morphology import erosion\n",
        "from enum import Enum\n",
        "\n",
        "square = np.array([\n",
        "    [1, 1, 1],\n",
        "    [1, 1, 1],\n",
        "    [1, 1, 1],\n",
        "])\n",
        "\n",
        "def multi_erode(im, num, element=square):  # TODO: Figure out if needed, this is currently unused.\n",
        "    for _ in range(num):\n",
        "        im = erosion(im, element)\n",
        "    return im\n",
        "\n",
        "def euclidean_distance(p1, p2):\n",
        "    return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)\n",
        "\n",
        "def zero_out_circle(target_mask, center, radius, inverse=False):\n",
        "    rows, cols = np.ogrid[:target_mask.shape[0], :target_mask.shape[1]]\n",
        "    circular_mask = (rows - center[1])**2 + (cols - center[0])**2 <= radius**2\n",
        "    target_mask[circular_mask] = 1 if inverse else 0\n",
        "    return target_mask\n",
        "\n",
        "class LoggingMode(Enum):\n",
        "    QUIET = 0\n",
        "    NORMAL = 1\n",
        "    VERBOSE = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "ocuKxliizQuy"
      },
      "outputs": [],
      "source": [
        "def pick_points_in_mask(mask, number, distance=0, positive=False, existing_points=[], logging=LoggingMode.QUIET):\n",
        "    target_mask = np.copy(mask)\n",
        "    input_points = []\n",
        "\n",
        "    if distance > 0:\n",
        "        for point in existing_points:\n",
        "            zero_out_circle(target_mask, point, distance, not positive)\n",
        "\n",
        "    while len(input_points) < number:\n",
        "        valid_indices = np.argwhere(target_mask != 0) if positive else np.argwhere(target_mask == 0)\n",
        "\n",
        "        if len(valid_indices) == 0:\n",
        "            if logging.value >= LoggingMode.NORMAL.value:\n",
        "                print(f\"Found only {len(input_points)} control points when {number} requested.\")\n",
        "                print(\"No valid points left.\")\n",
        "            break  # We have fewer than \"number\" points, but there are no valid points left.\n",
        "        # END IF\n",
        "\n",
        "        # TODO: Selection of point position could potentially be improved. If the point is too near\n",
        "        # the edge (judged by the distance to the nearest invalid index) then try picking again.\n",
        "        random_index = np.random.randint(0, len(valid_indices))\n",
        "        row, col = valid_indices[random_index]\n",
        "        new_point = [col, row]\n",
        "\n",
        "        input_points.append(new_point)\n",
        "        if distance > 0:\n",
        "            zero_out_circle(target_mask, new_point, distance, not positive)\n",
        "    # END WHILE\n",
        "\n",
        "    return np.array(input_points)\n",
        "\n",
        "def determine_dynamic_distance(mask, num_pts, positive=True, logging=LoggingMode.QUIET):\n",
        "    one_indices = np.array(mask != 0) if positive else np.array(mask == 0)\n",
        "    # ratio = one_indices.sum() / (mask.shape[0] * mask.shape[1])\n",
        "    # min_distance = ratio * min(mask.shape[0], mask.shape[1])\n",
        "    # min_distance = (one_indices.sum() ** 0.5) / 2\n",
        "\n",
        "    # Works better than the above linear version to have larger min distance values compared to linear\n",
        "    # for smaller ratios and to not let the min distance grow too much relative to linear for large ratios.\n",
        "    min_distance = 3 * (one_indices.sum() ** 0.5) / num_pts\n",
        "\n",
        "    if logging.value >= LoggingMode.VERBOSE.value:\n",
        "        print(f\"Total in mask sum is {one_indices.sum()}\")\n",
        "        print(f\"Using dynamic distance value of {min_distance}\")\n",
        "\n",
        "    return min_distance\n",
        "\n",
        "def generate_input_points(\n",
        "    number=None,\n",
        "    mask=None,\n",
        "    min_distance: int = 0,\n",
        "    dynamic_distance=False,\n",
        "    negative_src_mask=None,\n",
        "    num_negative=None,\n",
        "    positive_src_mask=None,\n",
        "    num_positive=None,\n",
        "    perturbation=0,  # TODO: Implement this! Would allow points outside of the true mask.\n",
        "    logging: LoggingMode = LoggingMode.QUIET,\n",
        "):\n",
        "    input_points = []\n",
        "    input_labels = []\n",
        "\n",
        "    min_distance_pos: float\n",
        "    min_distance_neg: float\n",
        "\n",
        "    _num_positive = num_positive if num_positive is not None else number\n",
        "    if _num_positive is not None:\n",
        "        src_mask = positive_src_mask if positive_src_mask is not None else mask\n",
        "        if dynamic_distance:\n",
        "            min_distance_pos = determine_dynamic_distance(src_mask, _num_positive, logging=logging)\n",
        "\n",
        "        control_points = pick_points_in_mask(\n",
        "            src_mask,\n",
        "            _num_positive,\n",
        "            distance=min_distance_pos,\n",
        "            positive=True,\n",
        "            logging=logging,\n",
        "        )\n",
        "        input_points.extend(control_points)\n",
        "        input_labels.extend([1] * len(control_points))\n",
        "\n",
        "        if len(input_points) < _num_positive:\n",
        "            pad_amnt = _num_positive - len(input_points)\n",
        "            input_points.extend([(0, 0)] * pad_amnt)\n",
        "            input_labels.extend([-10] * pad_amnt)\n",
        "\n",
        "            if logging.value >= LoggingMode.NORMAL.value:\n",
        "                print(\n",
        "                    f\"Found {len(input_points)} control points when {_num_positive} requested, padding with {pad_amnt} pad points.\")\n",
        "    # END POSITIVE CONTROL POINT PICKING\n",
        "\n",
        "    _num_negative = num_negative if num_negative is not None else number\n",
        "    if _num_negative is not None:\n",
        "        src_mask = negative_src_mask if negative_src_mask is not None else mask\n",
        "        if dynamic_distance:\n",
        "            min_distance_neg = determine_dynamic_distance(src_mask, _num_negative, positive=False, logging=logging)\n",
        "            min_distance_neg = max(min_distance_pos, min_distance_neg)\n",
        "\n",
        "        control_points = pick_points_in_mask(\n",
        "            src_mask,\n",
        "            _num_negative,\n",
        "            distance=min_distance_neg,\n",
        "            existing_points=input_points,\n",
        "            logging=logging,\n",
        "        )\n",
        "        input_points.extend(control_points)\n",
        "        input_labels.extend([0] * len(control_points))\n",
        "\n",
        "        if len(input_points) < _num_negative:\n",
        "            pad_amnt = _num_negative - len(input_points)\n",
        "            if logging.value >= LoggingMode.NORMAL.value:\n",
        "                print(f\"Found {len(input_points)} control points when {_num_negative} requested, padding with {pad_amnt} pad points.\")\n",
        "            \n",
        "            input_points.extend([(0, 0)] * pad_amnt)\n",
        "            input_labels.extend([-10] * pad_amnt)\n",
        "    # END NEGATIVE CONTROL POINT PICKING\n",
        "\n",
        "    input_points = np.array(input_points)\n",
        "    input_labels = np.array(input_labels)\n",
        "\n",
        "    return input_points, input_labels\n",
        "\n",
        "def get_input_bbox(mask, perturbation=0):\n",
        "    # Find minimum mask bounding all included mask points.\n",
        "    y_indices, x_indices = np.where(mask > 0)\n",
        "    x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
        "    y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
        "\n",
        "    if perturbation:  # Add perturbation to bounding box coordinates.\n",
        "        H, W = mask.shape\n",
        "        x_min = max(0, x_min + np.random.randint(-perturbation, perturbation))\n",
        "        x_max = min(W, x_max + np.random.randint(-perturbation, perturbation))\n",
        "        y_min = max(0, y_min + np.random.randint(-perturbation, perturbation))\n",
        "        y_max = min(H, y_max + np.random.randint(-perturbation, perturbation))\n",
        "\n",
        "    bbox = [x_min, y_min, x_max, y_max]\n",
        "    return bbox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zl9XdnQzQuz"
      },
      "source": [
        "TODO:\n",
        "* Should we erode the mask before picking training control points, the way we do at inference time usually? Perhaps just erode less than inference time?\n",
        "* Should we use a technique with erosion to find the main river portion, the subtract it from the original mask to get the smaller feeder streams? Could use this to place control points more strategically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "n6mecHeUzQuz"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9939c1a2b8a64e3a81a9a0fbcc8c621c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(Button(description='Prev', icon='arrow-left', style=ButtonStyle(), tooltip='Prev'), Button(desc…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f33118d7eec5428ba6b42ead081f6efc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(IntSlider(value=1, description='Datapoint:', max=197), ToggleButton(value=True, descript…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import ipywidgets as widgets\n",
        "import cv2\n",
        "\n",
        "datapoint_slider = widgets.IntSlider(\n",
        "    value=1, min=0, max=len(training_data) - 1, step=1,\n",
        "    description=\"Datapoint:\"\n",
        ")\n",
        "prev_button = widgets.Button(\n",
        "    description=\"Prev\",\n",
        "    tooltip=\"Prev\",\n",
        "    disabled=False,\n",
        "    button_style=\"\",\n",
        "    icon=\"arrow-left\"\n",
        ")\n",
        "next_button = widgets.Button(\n",
        "    description=\"Next\",\n",
        "    tooltip=\"Next\",\n",
        "    disabled=False,\n",
        "    button_style=\"\",\n",
        "    icon=\"arrow-right\"\n",
        ")\n",
        "\n",
        "def change_slider_value(diff):\n",
        "    next_value = datapoint_slider.value + diff\n",
        "    if next_value > datapoint_slider.min and next_value < datapoint_slider.max:\n",
        "        datapoint_slider.value = next_value\n",
        "\n",
        "def incrementSliderValue(_ignore): change_slider_value(1)\n",
        "def decrementSliderValue(_ignore): change_slider_value(-1)\n",
        "prev_button.on_click(decrementSliderValue)\n",
        "next_button.on_click(incrementSliderValue)\n",
        "\n",
        "trigger_refresh_btn = widgets.ToggleButton(\n",
        "    value=True,\n",
        "    description=\"Randomize\",\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "def handle_widgets_update(datapoint_index: int, _randomize):\n",
        "    sample_datapoint = training_data[datapoint_index]\n",
        "    sample_datapoint = {\n",
        "        \"image\": cv2.resize(np.array(sample_datapoint[\"image\"]), (256, 256)),\n",
        "        \"label\": cv2.resize(np.array(sample_datapoint[\"label\"]), (256, 256)),\n",
        "    }\n",
        "\n",
        "    # If you add verbose logging to these calls, the numbers should match those\n",
        "    # output by dynamic_distance=True calls to generate_input_points below.\n",
        "    min_distance_pos = determine_dynamic_distance(sample_datapoint[\"label\"], 5)  #, logging=LoggingMode.VERBOSE)\n",
        "    # min_distance_neg = determine_dynamic_distance(sample_datapoint[\"label\"], 0, positive=False)  #, logging=LoggingMode.VERBOSE)\n",
        "    # min_distance_neg = max(min_distance_pos, min_distance_neg)\n",
        "    min_distance_neg = None\n",
        "\n",
        "    points = generate_input_points(\n",
        "        num_positive=5,\n",
        "        # num_negative=0,\n",
        "        mask=sample_datapoint[\"label\"],\n",
        "        dynamic_distance=True,\n",
        "        logging=LoggingMode.VERBOSE\n",
        "    )\n",
        "    bbox = get_input_bbox(sample_datapoint[\"label\"], perturbation=10)\n",
        "    preview_datapoint(sample_datapoint, points=points, bbox=bbox, mask=True, min_distance=min_distance_pos, min_dist_negative=min_distance_neg)\n",
        "\n",
        "stepper_buttons = widgets.HBox([prev_button, next_button])\n",
        "interactive_plot = widgets.interactive(\n",
        "    handle_widgets_update,\n",
        "    datapoint_index=datapoint_slider,\n",
        "    _randomize=trigger_refresh_btn,\n",
        ")\n",
        "display(stepper_buttons, interactive_plot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TODO:\n",
        "\n",
        "Write an alternative point prompt picking algorithm. Take the positive source mask, then place a dynamic number of points such that each point covers is at least some distance from points outside the mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GNP1LFRozQuz"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class PromptType:\n",
        "    CONTROL_POINTS = \"pts\"\n",
        "    BOUNDING_BOX = \"bbox\"\n",
        "\n",
        "class SAMDataset(Dataset):\n",
        "    def __init__(self, dataset, processor, prompt_type = PromptType.CONTROL_POINTS):\n",
        "        self.dataset = dataset\n",
        "        self.processor = processor\n",
        "        self.prompt_type = prompt_type\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        datapoint = self.dataset[idx]\n",
        "        input_image = cv2.resize(np.array(datapoint[\"image\"]), (256, 256)) # This doesn't need to be 256x256 per se.\n",
        "        ground_truth_mask = cv2.resize(np.array(datapoint[\"label\"]), (256, 256)) # This needs to be 256x256 since that is the size of model outputs.\n",
        "\n",
        "        if self.prompt_type == PromptType.CONTROL_POINTS:\n",
        "            inputs = self._getitem_ctrlpts(input_image, ground_truth_mask)\n",
        "        else:\n",
        "            inputs = self._getitem_bbox(input_image, ground_truth_mask)\n",
        "\n",
        "        # Add ground truth segmentation.\n",
        "        inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
        "\n",
        "        return inputs\n",
        "\n",
        "    def _getitem_ctrlpts(self, input_image, ground_truth_mask):\n",
        "        # Get control points prompt.\n",
        "        input_points, input_labels = generate_input_points(\n",
        "            num_positive=5,\n",
        "            mask=ground_truth_mask,\n",
        "            dynamic_distance=True,\n",
        "            logging=LoggingMode.QUIET\n",
        "        )\n",
        "        input_points = input_points.astype(float).tolist()\n",
        "        input_labels = input_labels.tolist()\n",
        "        input_labels = [[x] for x in input_labels]\n",
        "\n",
        "        # Prepare the image and prompt for the model.\n",
        "        inputs = self.processor(\n",
        "            input_image,\n",
        "            input_points=input_points,\n",
        "            input_labels=input_labels,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Remove batch dimension which the processor adds by default.\n",
        "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "        inputs[\"input_labels\"] = inputs[\"input_labels\"].squeeze(1)\n",
        "        # inputs[\"input_points\"] = inputs[\"input_points\"].squeeze(1)\n",
        "\n",
        "        return inputs\n",
        "\n",
        "    def _getitem_bbox(self, input_image, ground_truth_mask):\n",
        "        # Get bounding box prompt.\n",
        "        bbox = get_input_bbox(ground_truth_mask, perturbation=10)\n",
        "\n",
        "        # Prepare the image and prompt for the model.\n",
        "        inputs = self.processor(input_image, input_boxes=[[bbox]], return_tensors=\"pt\")\n",
        "        inputs = {k: v.squeeze(0) for k, v in inputs.items()} # Remove batch dimension which the processor adds by default.\n",
        "\n",
        "        return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IttseR9AzQu0",
        "outputId": "0b3d8120-f8b9-46e8-c675-f9faeccc98a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded facebook/sam-vit-base model processor\n"
          ]
        }
      ],
      "source": [
        "from transformers import SamProcessor\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "model_size = \"base\"\n",
        "# model_size = \"huge\"\n",
        "model_name = f\"facebook/sam-vit-{model_size}\"\n",
        "\n",
        "processor = SamProcessor.from_pretrained(model_name)\n",
        "print(f\"Loaded {model_name} model processor\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iIN29qVzQu0",
        "outputId": "5f7cdc98-5232-46f1-b352-74530d1912c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using batch size of 8 with bbox prompts\n"
          ]
        }
      ],
      "source": [
        "batch_size = 7\n",
        "prompt_type = PromptType.BOUNDING_BOX\n",
        "print(f\"Using batch size of {batch_size} with {prompt_type} prompts\")\n",
        "\n",
        "train_dataset = SAMDataset(dataset=training_data, processor=processor, prompt_type=prompt_type)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "validation_dataset = SAMDataset(dataset=validation_data, processor=processor, prompt_type=prompt_type)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FMAaXR_zQu0",
        "outputId": "19b7cbba-2145-481b-f621-e86fa9e85a5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pixel_values torch.Size([3, 1024, 1024])\n",
            "original_sizes torch.Size([2])\n",
            "reshaped_input_sizes torch.Size([2])\n",
            "input_boxes torch.Size([1, 4])\n",
            "ground_truth_mask (256, 256)\n"
          ]
        }
      ],
      "source": [
        "example = train_dataset[0]\n",
        "for k, v in example.items():\n",
        "    print(k, v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od8UdFAazQu0",
        "outputId": "93f6358d-a993-4fd0-b506-6efb1145a175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pixel_values torch.Size([8, 3, 1024, 1024])\n",
            "original_sizes torch.Size([8, 2])\n",
            "reshaped_input_sizes torch.Size([8, 2])\n",
            "input_boxes torch.Size([8, 1, 4])\n",
            "ground_truth_mask torch.Size([8, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "for k, v in batch.items():\n",
        "    print(k, v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MiWYeWzRnmaL"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15) # Clip predictions to avoid log(0) errors\n",
        "    loss = -y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred) # Calculate cross-entropy loss for each data point\n",
        "    return np.mean(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiDyjde_zQu1",
        "outputId": "a9034b97-2e7f-475d-8cdd-e3c1321b4bb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded facebook/sam-vit-base model, using AdamW optimizer\n"
          ]
        }
      ],
      "source": [
        "from transformers import SamModel\n",
        "from torch.optim import AdamW, Optimizer\n",
        "from typing import Any\n",
        "import monai\n",
        "\n",
        "model: Any = SamModel.from_pretrained(model_name)\n",
        "\n",
        "# Note: Hyperparameter tuning could improve performance here\n",
        "learning_rate = 5e-6  # 1e-5\n",
        "weight_decay = 1e-4\n",
        "optimizer_name = \"AdamW\"  # Make sure to change this if changing optimizer!\n",
        "\n",
        "optimizer = AdamW(model.mask_decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "lossFn = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction=\"mean\")  # type: ignore\n",
        "\n",
        "print(f\"Loaded {model_name} model, using {optimizer_name} optimizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gw3ZFxatzQu1"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
        "        param.requires_grad_(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzbQbYuC09-Z",
        "outputId": "105a063c-98a7-45d2-d2a0-b70deaf65d1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Save checkpoint path: ./checkpoints/base_AdamW_lr=5e-06_wd=0.0001_bs=8_bbox.pth\n",
            "Loading from checkpoint? False\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p \"./checkpoints\"\n",
        "hyperparams = f\"lr={learning_rate}_wd={weight_decay}_bs={batch_size}_{prompt_type}\"\n",
        "checkpoint_path = f\"./checkpoints/{model_size}_{optimizer_name}_{hyperparams}.pth\"\n",
        "\n",
        "load_checkpoint = False\n",
        "print(\"Save checkpoint path:\", checkpoint_path)\n",
        "print(\"Loading from checkpoint?\", load_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbJDvGKX1xyX"
      },
      "source": [
        "If uploading a `.pth` checkpoint to continue from on Google Colab, make sure to wait until it is done uploading before continuing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5g3IwyJU4pCy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "num_epochs = 300\n",
        "save_every = 50\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "checkpoint = None\n",
        "if load_checkpoint:\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "    print(f\"Model checkpoint loaded from: {checkpoint_path}\")\n",
        "    print(f\"Resuming training from epoch: {checkpoint['epoch']} of {num_epochs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "oCjrpzluPLYt"
      },
      "outputs": [],
      "source": [
        "def save_model_checkpoint(model: SamModel, optimizer: Optimizer, epoch: int, best_loss: float, train_losses, validation_losses):\n",
        "    print(f\"Saving checkpoint for epoch {epoch}...\")\n",
        "    checkpoint = {\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"best_loss\": best_loss,\n",
        "        \"train_losses\": train_losses,\n",
        "        \"validation_losses\": validation_losses,\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U293ARIxrX8p"
      },
      "source": [
        "11.2 / 15.0 GB\n",
        "\n",
        "13.8 / 15.0 GB\n",
        "\n",
        "14.2 / 15.0 GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "r6BO5iipfGOj"
      },
      "outputs": [],
      "source": [
        "use_amp = True\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "UeC0FlIzPLYu"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "from tqdm import tqdm\n",
        "\n",
        "# For some reason torch.autocast throws an error if the device\n",
        "# is provided as a torch.device instead of a string...\n",
        "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def forward_pass(model: SamModel, batch):\n",
        "    with torch.autocast(device_type=device_str, dtype=torch.float16, enabled=use_amp):\n",
        "        if prompt_type == PromptType.CONTROL_POINTS:\n",
        "            outputs = model(\n",
        "                pixel_values=batch[\"pixel_values\"].to(device),\n",
        "                input_points=batch[\"input_points\"].to(device),\n",
        "                input_labels=batch[\"input_labels\"].to(device),\n",
        "                multimask_output=False,\n",
        "            )\n",
        "        elif prompt_type == PromptType.BOUNDING_BOX:\n",
        "            outputs = model(\n",
        "                pixel_values=batch[\"pixel_values\"].to(device),\n",
        "                input_boxes=batch[\"input_boxes\"].to(device),\n",
        "                multimask_output=False,\n",
        "            )\n",
        "    return outputs\n",
        "\n",
        "def calculate_loss(model: SamModel, prediction_masks: torch.Tensor, ground_truth_masks: torch.Tensor):\n",
        "    with torch.autocast(device_type=device_str, dtype=torch.float16, enabled=use_amp):\n",
        "        if prompt_type == PromptType.CONTROL_POINTS:\n",
        "            # For some reason, the output contains duplicate masks, seemingly one for each control\n",
        "            # point? We can just select the the first and compute loss for that, there is no difference.\n",
        "            loss = lossFn(prediction_masks[:, 0], ground_truth_masks)\n",
        "        elif prompt_type == PromptType.BOUNDING_BOX:\n",
        "            loss = lossFn(prediction_masks, ground_truth_masks)\n",
        "    return scaler.scale(loss)\n",
        "\n",
        "def evaluate_model(model: SamModel, dataloader: DataLoader) -> torch.Tensor:\n",
        "    model.eval()\n",
        "    losses: list[torch.Tensor] = []\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Eval\"):\n",
        "        # Forward Pass\n",
        "        with torch.no_grad():\n",
        "            outputs = forward_pass(model, batch)\n",
        "\n",
        "        # Compute Loss\n",
        "        predicted_masks = outputs.pred_masks.squeeze((1, 2))\n",
        "        ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
        "\n",
        "        loss = calculate_loss(model, predicted_masks, ground_truth_masks)\n",
        "        losses.append(loss)\n",
        "\n",
        "        # Avoid Memory Overload\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    # END FOR\n",
        "\n",
        "    return sum(losses) / len(losses)  # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xD0rktREzQu1"
      },
      "outputs": [],
      "source": [
        "from statistics import mean\n",
        "\n",
        "def train_model(model: SamModel, dataloader: DataLoader, checkpoint=None, eval_every=5):\n",
        "    model.train()\n",
        "\n",
        "    epoch = 0\n",
        "    best_loss = float(\"inf\")\n",
        "    train_losses = []\n",
        "    validation_losses = []\n",
        "\n",
        "    if checkpoint:\n",
        "        epoch = checkpoint[\"epoch\"] + 1  # Start from the next epoch\n",
        "        best_loss = checkpoint[\"best_loss\"]\n",
        "        train_losses = checkpoint[\"train_losses\"]\n",
        "        validation_losses = checkpoint[\"validation_losses\"]\n",
        "        print(f\"Resuming from checkpoint at epoch {checkpoint['epoch']}\")\n",
        "\n",
        "    while epoch < num_epochs:\n",
        "        epoch_losses = []\n",
        "\n",
        "        for batch in tqdm(dataloader, desc=\"Train Batch\"):\n",
        "            # Forward Pass\n",
        "            outputs = forward_pass(model, batch)\n",
        "\n",
        "            # Compute Loss\n",
        "            predicted_masks = outputs.pred_masks.squeeze((1, 2))\n",
        "            ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
        "            loss = calculate_loss(model, predicted_masks, ground_truth_masks)\n",
        "\n",
        "            # Backward Pass & Optimizer Step\n",
        "            # loss.backward()\n",
        "            # optimizer.step()\n",
        "            # optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            epoch_losses.append(loss.item())\n",
        "        # END FOR\n",
        "\n",
        "        if epoch % eval_every == 0:\n",
        "            # TODO: train_dataloader is passed as a function arg, should validation_dataloader be passed too?\n",
        "            validation_loss = evaluate_model(model, validation_dataloader)\n",
        "            validation_losses.append(validation_loss.item())\n",
        "\n",
        "        mean_loss = mean(epoch_losses)\n",
        "        if epoch % save_every == 0 or epoch == num_epochs - 1:\n",
        "            if mean_loss < best_loss:\n",
        "                save_model_checkpoint(model, optimizer, epoch, mean_loss, train_losses, validation_losses)\n",
        "                best_loss = mean_loss  # TODO: Should we evaluate on validation every epoch, then save based on best validation loss?\n",
        "            else:\n",
        "                print(f\"Skipping checkpoint save since prev checkpoint is better: {best_loss} < {mean_loss}\")\n",
        "        # END IF\n",
        "\n",
        "        print(f\"Epoch: {epoch + 1}/{num_epochs}\")\n",
        "        print(f\"Mean loss: {mean_loss}\")\n",
        "        train_losses.append(mean_loss)\n",
        "        epoch += 1\n",
        "    # END WHILE\n",
        "\n",
        "    return train_losses, validation_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "nr3vsrEfwgOm"
      },
      "outputs": [],
      "source": [
        "# Evaluate baseline train data performance:\n",
        "# print(\"Average train loss:\", evaluate_model(model, train_dataloader))\n",
        "\n",
        "# Evaluate zero-shot model performance:\n",
        "# print(\"\\nAverage validation loss:\", evaluate_model(model, validation_dataloader))\n",
        "\n",
        "# TODO: Probably include the test set here in the future too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDHqT_yUdrcG",
        "outputId": "b0e5b13e-280a-4495-deee-39d7467c5d3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training started at 02:59 03/06/2024\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "current_datetime = datetime.now()\n",
        "print(f\"Training started at {current_datetime.strftime('%H:%M %m/%d/%Y')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "-oqcJcXAcEzd",
        "outputId": "ae683e1a-9129-481b-8469-5bc6500c6b85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Batch:   0%|          | 0/26 [00:02<?, ?it/s]\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 14.75 GiB of which 4.66 GiB is free. Process 24352 has 10.09 GiB memory in use. Of the allocated memory 3.94 GiB is allocated by PyTorch, and 6.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-efbf0d5fd71d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0meval_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-48b6fd6c9838>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, checkpoint, eval_every)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Train Batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Forward Pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# Compute Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-79dbcdba3787>\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, batch)\u001b[0m\n\u001b[1;32m     16\u001b[0m             )\n\u001b[1;32m     17\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mprompt_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPromptType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBOUNDING_BOX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             outputs = model(\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pixel_values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0minput_boxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_boxes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/sam/modeling_sam.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, input_points, input_labels, input_boxes, input_masks, image_embeddings, multimask_output, attention_similarity, target_embedding, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpixel_values\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m             vision_outputs = self.vision_encoder(\n\u001b[0m\u001b[1;32m   1362\u001b[0m                 \u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/sam/modeling_sam.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1048\u001b[0m                 )\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m                 \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/sam/modeling_sam.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_partition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         hidden_states, attn_weights = self.attn(\n\u001b[0m\u001b[1;32m    944\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/sam/modeling_sam.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m    848\u001b[0m             )\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0mattn_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1856\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1858\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1859\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 14.75 GiB of which 4.66 GiB is free. Process 24352 has 10.09 GiB memory in use. Of the allocated memory 3.94 GiB is allocated by PyTorch, and 6.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "eval_every = 5\n",
        "train_losses, validation_losses = train_model(model, train_dataloader, checkpoint, eval_every=eval_every)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRfqRCxaNRWK"
      },
      "outputs": [],
      "source": [
        "val_loss_epochs = [i * eval_every for i in range(len(validation_losses))]\n",
        "\n",
        "plt.plot(train_losses, label=\"Train\")\n",
        "plt.plot(val_loss_epochs, validation_losses, label=\"Validation\")\n",
        "plt.title(f\"Epoch vs Loss (model={model_size}, optim={optimizer_name}, lr={learning_rate}, wd={weight_decay}, bs={batch_size}, {prompt_type})\")\n",
        "plt.xlabel(\"Training Epoch #\")\n",
        "plt.ylabel(\"Mean Epoch Train Loss\")\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-XaLX6uJiLV"
      },
      "source": [
        "| Model        | Optimizer | Learning Rate   | Weight Decay | Batch Size | Epochs | Prompts | Train Loss | Validation Loss |\n",
        "|--------------|-----------|-----------------|--------------|------------|--------|---------|------------|-----------------|\n",
        "| sam-vit-base | AdamW     | 1e-5            | 0            | 2          | 300    | points  | 220.9510   | 235.2056        |\n",
        "| sam-vit-base | AdamW     | 3e-5            | 0            | 3          | 300    | points  | 220.3394   | 245.4707        |\n",
        "| sam-vit-base | AdamW     | 7e-6            | 0            | 3          | 600    | points  | 220.4751   | 223.8311        |\n",
        "| sam-vit-base | AdamW     | 7e-6            | 1e-4         | 4          | 300    | points  | 222.0167   | 219.9485        |\n",
        "| sam-vit-base | AdamW     | 7e-6            | 1e-5         | 5          | 200    | points  | 224.3772   | 220.1452        |\n",
        "| sam-vit-base | AdamW     | 7e-6            | 2e-4         | 5          | 300    | points  | 224.0496   | 223.9057        |\n",
        "| sam-vit-base | AdamW     | 7e-6            | 2e-4         | 5          | 300    | bboxes  | 224.3229   | 217.7175        |\n",
        "| sam-vit-huge | AdamW     | 1e-5            | 0            | 2          | 200    | points  | 221.4048   | 255.4305        |\n",
        "| sam-vit-huge | AdamW     | 7e-6            | 1e-4         | 3          | 300    | points  | 225.3418   | 213.9504        |\n",
        "| sam-vit-huge | AdamW     | 5e-6            | 1e-4         | 3          | 300    | bboxes  | 223.7908   | 215.4390        |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsEWMgKdJPnY"
      },
      "outputs": [],
      "source": [
        "# Evaluate performance after training:\n",
        "print(\"Average train loss:\", evaluate_model(model, train_dataloader))\n",
        "print(\"\\nAverage validation loss:\", evaluate_model(model, validation_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9N8X53uWPLYv"
      },
      "outputs": [],
      "source": [
        "def visualize_prediction_quality(loss, raw_prediction, final_mask, ground_truth, input_points = None, input_labels = None, input_boxes = None):\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 10))\n",
        "    overlay_mask(raw_prediction, axes[0])\n",
        "    overlay_mask(final_mask, axes[1])\n",
        "    axes[2].imshow(ground_truth)\n",
        "\n",
        "    for ax in axes: ax.axis(\"off\")\n",
        "    axes[0].title.set_text(f\"loss={loss}\")\n",
        "\n",
        "    if input_points is not None and input_labels is not None:\n",
        "        show_control_points(input_points / 4, input_labels, axes[1])\n",
        "        # To also display prompt overlay on the ground truth mask:\n",
        "        # show_control_points(input_points / 4, input_labels, axes[2])\n",
        "\n",
        "    if input_boxes is not None:\n",
        "        show_bounding_box(input_boxes / 4, axes[1])\n",
        "        # show_bounding_box(input_boxes, axes[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbknISjxAtkF"
      },
      "outputs": [],
      "source": [
        "def visually_evaluate_model(model: SamModel, dataloader: DataLoader, displayCount: int = float(\"inf\")):\n",
        "    model.eval()\n",
        "    iterCount = 0\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Eval\"):\n",
        "        # Forward Pass\n",
        "        with torch.no_grad():\n",
        "            outputs = forward_pass(model, batch)\n",
        "\n",
        "        outputs = outputs.pred_masks.squeeze((1, 2))\n",
        "        predicted_masks = torch.sigmoid(outputs)\n",
        "        ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
        "\n",
        "        if prompt_type == PromptType.CONTROL_POINTS:\n",
        "            outputs = outputs[:, 0]\n",
        "            predicted_masks = predicted_masks[:, 0]\n",
        "\n",
        "        for batch_item in range(predicted_masks.shape[0]):\n",
        "            loss = lossFn(outputs[batch_item], ground_truth_masks[batch_item])\n",
        "\n",
        "            raw_mask = predicted_masks[batch_item].cpu().numpy()\n",
        "            mask = (raw_mask > 0.5).astype(np.uint8)  # TODO: Replace 0.5 with variable confidence threshold.\n",
        "\n",
        "            if prompt_type == PromptType.CONTROL_POINTS:\n",
        "                visualize_prediction_quality(\n",
        "                    loss,\n",
        "                    raw_mask,\n",
        "                    mask,\n",
        "                    batch[\"ground_truth_mask\"][batch_item],\n",
        "                    input_points=batch[\"input_points\"][batch_item],\n",
        "                    input_labels=batch[\"input_labels\"][batch_item],\n",
        "                )\n",
        "            elif prompt_type == PromptType.BOUNDING_BOX:\n",
        "                visualize_prediction_quality(\n",
        "                    loss,\n",
        "                    raw_mask,\n",
        "                    mask,\n",
        "                    batch[\"ground_truth_mask\"][batch_item],\n",
        "                    input_boxes=batch[\"input_boxes\"][batch_item, 0],\n",
        "                )\n",
        "\n",
        "        iterCount += 1\n",
        "        if iterCount >= displayCount:\n",
        "            break\n",
        "    # END FOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Brm27YGnM4Ye"
      },
      "outputs": [],
      "source": [
        "visually_evaluate_model(model, validation_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hzvb7YqJRckt"
      },
      "outputs": [],
      "source": [
        "visually_evaluate_model(model, validation_dataloader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "475a08db99efb31e89a9087e9ca2d38d56797a16dd41a0621d20f3dc95701503"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
