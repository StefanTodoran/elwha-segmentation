{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjfwt2MCzQuu"
      },
      "source": [
        "# River Boundary Masking\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Cold water refuges are crucial havens for fish species, offering them sanctuary and relief. These refuges are specific areas within aquatic ecosystems, such as deep pools or shaded sections, where the water remains relatively colder than the surrounding environment. With the increasing impact of climate change and its associated rising temperatures, cold water refuges serve as essential habitats for fish, providing them with a reprieve from stressful conditions and enabling their survival and reproductive success. By seeking out these refuges, fish can maintain optimal physiological functioning, evade thermal stress, and ensure the long-term sustainability of their populations.\n",
        "\n",
        "In this section, we will use our realigned dataset to create river masks for our IR images. These masks will then be used to identify cold water refuges in the river in the next section. The segmentation model selected for use is Meta's SAM. The workflow overview of how it is utilized to create river masks is as follows:\n",
        "\n",
        "* Create a ground truth dataset by hand using paint.net\n",
        "* Create mechanism for creating point prompts for SAM\n",
        "* Evaluate base model performance and generate masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au1BDVv6zQuv"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The first step is to build our ground truth dataset. It seems that the original processing done in 2012 included some preliminary river masks. Unfortunately, these masks are fairly poor. They also do not help with the construction of our ground truth dataset; it is easier to just manually create new masks than it is to patch up the existing masks.\n",
        "\n",
        "The process of creating ground truth masks involves using the magic wand tool in paint.net, which selects an contiguous area of similar color based on some tolerance value. Then this area is filled with black and the rest of the image with white, and the mask can then be manually adjusted to match the river in areas where the magic wand fails to align perfectly with the river edge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47_I-zIyzQuw"
      },
      "source": [
        "The optimal results are achieved by using RGB images layered with transparency over the IR images, as the IR images have the highest contrast between river and non-river pixels. This is because reflectance of water is lower than soil and vegetation across a wide range of the electromagnetic spectrum, including within the visible RGB portion, but the difference is most significant in the IR range.\n",
        "\n",
        "![Quadrant Filtering](https://github.com/geo-smart/water-surf/blob/main/book/img/reflectance.png?raw=1)\n",
        "\n",
        "The difference in reflectance then results in the contrast between the river and land pixels being larger in the IR images. This also means that while both the RGB and IR remote sensing imagery may prove relevant to the model, the IR images will be the key to accurate segmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6MqhHWcHW3sG"
      },
      "outputs": [],
      "source": [
        "# !pip install -q torch transformers datasets tqdm monai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uMd2zYm_W3sG"
      },
      "outputs": [],
      "source": [
        "# !pip install -q huggingface_hub\n",
        "fetch_updated = False\n",
        "\n",
        "# if fetch_updated:\n",
        "#   !huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMIPy6lNzQuw",
        "outputId": "c9044219-6a0a-4dc4-90eb-c17f65355a8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from disk...\n",
            "Dataset({\n",
            "    features: ['image', 'label'],\n",
            "    num_rows: 203\n",
            "})\n",
            "Dataset({\n",
            "    features: ['image', 'label'],\n",
            "    num_rows: 22\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, load_from_disk, Dataset\n",
        "\n",
        "training_data_path = \"./elwha-dataset-tiny-train.hf\"\n",
        "validation_data_path = \"./elwha-dataset-tiny-val.hf\"\n",
        "\n",
        "if fetch_updated:\n",
        "  print(\"Fetchng data from huggingface server...\")\n",
        "  training_data: Dataset = load_dataset(\"stodoran/elwha-segmentation-tiny\", split=\"train\") # type: ignore\n",
        "  validation_data: Dataset = load_dataset(\"stodoran/elwha-segmentation-tiny\", split=\"validation\") # type: ignore\n",
        "  training_data.save_to_disk(training_data_path)\n",
        "  validation_data.save_to_disk(validation_data_path)\n",
        "else:\n",
        "  print(\"Loading data from disk...\")\n",
        "  training_data = load_from_disk(training_data_path) # type: ignore\n",
        "  validation_data = load_from_disk(validation_data_path) # type: ignore\n",
        "\n",
        "print(training_data)\n",
        "print(validation_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "u5V5Q32RzQux"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Circle\n",
        "\n",
        "def overlay_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "def show_control_points(coords, labels, ax, min_distance=None, marker_size=375):\n",
        "    if len(coords) == 0 or len(labels) == 0: return\n",
        "    pos_points = coords[labels==1]\n",
        "    neg_points = coords[labels==0]\n",
        "\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "\n",
        "    if min_distance is not None:\n",
        "        for point in pos_points:\n",
        "            circle = Circle((point[0], point[1]), min_distance, color=\"white\", fill=False, linestyle=\"dashed\")\n",
        "            ax.add_patch(circle)\n",
        "        \n",
        "        for point in neg_points:\n",
        "            circle = Circle((point[0], point[1]), min_distance, color=\"white\", fill=False, linestyle=\"dashed\")\n",
        "            ax.add_patch(circle)\n",
        "\n",
        "        ax.set_xlim(xlim)\n",
        "        ax.set_ylim(ylim)\n",
        "\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color=\"green\", marker=\"P\", s=marker_size, edgecolor=\"white\", linewidth=1.25)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color=\"red\", marker=\"X\", s=marker_size, edgecolor=\"white\", linewidth=1.25)\n",
        "\n",
        "def show_bounding_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor=\"green\", facecolor=(0,0,0,0), lw=2)) # type: ignore\n",
        "\n",
        "def preview_datapoint(datapoint, points=None, bbox=None, mask=True, min_distance=None):\n",
        "    fig, axes = plt.subplots()\n",
        "\n",
        "    axes.imshow(datapoint[\"image\"])\n",
        "    if mask: overlay_mask(np.array(datapoint[\"label\"]), axes)\n",
        "    if points: show_control_points(*points, axes, min_distance) # type: ignore\n",
        "    if bbox: show_bounding_box(bbox, axes)\n",
        "\n",
        "    axes.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oIVzzjJkzQuy"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CEzgXAgzmVaU"
      },
      "outputs": [],
      "source": [
        "from skimage.morphology import erosion\n",
        "from enum import Enum\n",
        "\n",
        "class LoggingMode(Enum):\n",
        "  QUIET = 0\n",
        "  NORMAL = 1\n",
        "  VERBOSE = 2\n",
        "\n",
        "square = np.array([\n",
        "  [1,1,1],\n",
        "  [1,1,1],\n",
        "  [1,1,1],\n",
        "])\n",
        "\n",
        "# TODO: Figure out if needed, this is currently unused.\n",
        "def multi_erod(im, num, element=square):\n",
        "  for _ in range(num):\n",
        "    im = erosion(im, element)\n",
        "  return im\n",
        "\n",
        "def euclidean_distance(p1, p2):\n",
        "  return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)\n",
        "\n",
        "def zero_out_circle(target_mask, center, radius):\n",
        "  rows, cols = np.ogrid[:target_mask.shape[0], :target_mask.shape[1]]\n",
        "  circular_mask = (rows - center[1])**2 + (cols - center[0])**2 <= radius**2\n",
        "  target_mask[circular_mask] = 0\n",
        "  return target_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ocuKxliizQuy"
      },
      "outputs": [],
      "source": [
        "def pick_points_in_mask(mask, number, distance=0, positive=False, existing_points=[], logging=LoggingMode.QUIET):\n",
        "  target_mask = np.copy(mask)\n",
        "  input_points = []\n",
        "\n",
        "  if distance > 0:\n",
        "    for point in existing_points:\n",
        "      zero_out_circle(target_mask, point, distance)\n",
        "\n",
        "  while len(input_points) < number:\n",
        "    valid_indices = np.argwhere(target_mask != 0) if positive else np.argwhere(target_mask == 0)\n",
        "    \n",
        "    if len(valid_indices) == 0:\n",
        "      if logging.value >= LoggingMode.NORMAL.value:\n",
        "        print(f\"Found only {len(input_points)} control points when {number} requested.\")\n",
        "        print(\"No valid points left.\")\n",
        "      break  # We have fewer than \"number\" points, but there are no valid points left.\n",
        "    # END IF\n",
        "\n",
        "    random_index = np.random.randint(0, len(valid_indices))\n",
        "    row, col = valid_indices[random_index]\n",
        "    new_point = [col, row]\n",
        "\n",
        "    input_points.append(new_point)\n",
        "    if distance > 0: zero_out_circle(target_mask, new_point, distance)\n",
        "  # END WHILE\n",
        "\n",
        "  return np.array(input_points)\n",
        "\n",
        "def determine_dynamic_distance(mask, positive=True, logging=LoggingMode.QUIET):\n",
        "  one_indices = np.array(mask != 0) if positive else np.array(mask == 0)\n",
        "  # ratio = one_indices.sum() / (mask.shape[0] * mask.shape[1])\n",
        "  # min_distance = ratio * min(mask.shape[0], mask.shape[1])\n",
        "  \n",
        "  min_distance = (one_indices.sum() ** 0.5) / 2\n",
        "  # min_distance = (one_indices.sum() ** 0.75) / 20\n",
        "  # min_distance = (one_indices.sum() ** 0.65) / 7.5\n",
        "\n",
        "  if logging.value >= LoggingMode.VERBOSE.value:\n",
        "    print(f\"Total in mask sum is {one_indices.sum()}\")\n",
        "    print(f\"Using dynamic distance value of {min_distance}\")\n",
        "\n",
        "  return min_distance\n",
        "\n",
        "def generate_input_points(\n",
        "  number = None,\n",
        "  mask = None,\n",
        "  min_distance: int = 0,\n",
        "  dynamic_distance = False,\n",
        "  negative_src_mask = None,\n",
        "  num_negative = None,\n",
        "  positive_src_mask = None,\n",
        "  num_positive = None,\n",
        "  perturbation = 0, # TODO: Implement this! Would allow points outside of the true mask.\n",
        "  logging: LoggingMode = LoggingMode.QUIET,\n",
        "):\n",
        "  input_points = []\n",
        "  input_labels = []\n",
        "\n",
        "  _num_positive = num_positive if num_positive is not None else number\n",
        "  if _num_positive is not None:\n",
        "    src_mask = positive_src_mask if positive_src_mask is not None else mask\n",
        "    if dynamic_distance: min_distance = determine_dynamic_distance(src_mask)\n",
        "    \n",
        "    control_points = pick_points_in_mask(\n",
        "      src_mask,\n",
        "      _num_positive,\n",
        "      distance=min_distance,\n",
        "      positive=True,\n",
        "      logging=logging,\n",
        "    )\n",
        "    input_points.extend(control_points)\n",
        "    input_labels.extend([1] * len(control_points))\n",
        "\n",
        "    if len(input_points) < _num_positive:\n",
        "      pad_amnt = _num_positive - len(input_points)\n",
        "      input_points.extend([(0,0)] * pad_amnt)\n",
        "      input_labels.extend([-10] * pad_amnt)\n",
        "\n",
        "      if logging.value >= LoggingMode.NORMAL.value: \n",
        "        print(f\"Found {len(input_points)} control points when {_num_positive} requested, padding with {pad_amnt} pad points.\")\n",
        "  # END POSITIVE CONTROL POINT PICKING\n",
        "\n",
        "  _num_negative = num_negative if num_negative is not None else number\n",
        "  if _num_negative is not None:\n",
        "    src_mask = negative_src_mask if negative_src_mask is not None else mask\n",
        "    if dynamic_distance: min_distance = determine_dynamic_distance(src_mask)\n",
        "    \n",
        "    control_points = pick_points_in_mask(\n",
        "      src_mask,\n",
        "      _num_negative,\n",
        "      distance=min_distance,\n",
        "      existing_points=input_points,\n",
        "      logging=logging,\n",
        "    )\n",
        "    input_points.extend(control_points)\n",
        "    input_labels.extend([0] * len(control_points))\n",
        "\n",
        "    if len(input_points) < _num_negative:\n",
        "      pad_amnt = _num_negative - len(input_points)\n",
        "      input_points.extend([(0,0)] * pad_amnt)\n",
        "      input_labels.extend([-10] * pad_amnt)\n",
        "\n",
        "      if logging.value >= LoggingMode.NORMAL.value: \n",
        "        print(f\"Found {len(input_points)} control points when {_num_negative} requested, padding with {pad_amnt} pad points.\")\n",
        "  # END NEGATIVE CONTROL POINT PICKING\n",
        "\n",
        "  input_points = np.array(input_points)\n",
        "  input_labels = np.array(input_labels)\n",
        "\n",
        "  return input_points, input_labels\n",
        "\n",
        "def get_input_bbox(mask, perturbation=0):\n",
        "  # Find minimum mask bounding all included mask points.\n",
        "  y_indices, x_indices = np.where(mask > 0)\n",
        "  x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
        "  y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
        "\n",
        "  if perturbation: # Add perturbation to bounding box coordinates.\n",
        "    H, W = mask.shape\n",
        "    x_min = max(0, x_min + np.random.randint(-perturbation, perturbation))\n",
        "    x_max = min(W, x_max + np.random.randint(-perturbation, perturbation))\n",
        "    y_min = max(0, y_min + np.random.randint(-perturbation, perturbation))\n",
        "    y_max = min(H, y_max + np.random.randint(-perturbation, perturbation))\n",
        "\n",
        "  bbox = [x_min, y_min, x_max, y_max]\n",
        "  return bbox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zl9XdnQzQuz"
      },
      "source": [
        "TODO: \n",
        "* Should we erode the mask before picking training control points, the way we do at inference time usually? Perhaps just erode less than inference time?\n",
        "* Should we use a technique with erosion to find the main river portion, the subtract it from the original mask to get the smaller feeder streams? Could use this to place control points more strategically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "n6mecHeUzQuz"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c6eefd69f14449096458b0a140966db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(Button(description='Prev', icon='arrow-left', style=ButtonStyle(), tooltip='Prev'), Button(desc…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2fdab9f96a1e4a5884755b455f3e40a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(IntSlider(value=1, description='Datapoint:', max=202), ToggleButton(value=True, descript…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import ipywidgets as widgets\n",
        "import cv2\n",
        "\n",
        "datapoint_slider = widgets.IntSlider(\n",
        "    value=1, min=0, max=len(training_data) - 1, step=1,\n",
        "    description=\"Datapoint:\"\n",
        ")\n",
        "prev_button = widgets.Button(\n",
        "    description=\"Prev\",\n",
        "    tooltip=\"Prev\",\n",
        "    disabled=False,\n",
        "    button_style=\"\",\n",
        "    icon=\"arrow-left\"\n",
        ")\n",
        "next_button = widgets.Button(\n",
        "    description=\"Next\",\n",
        "    tooltip=\"Next\",\n",
        "    disabled=False,\n",
        "    button_style=\"\",\n",
        "    icon=\"arrow-right\"\n",
        ")\n",
        "\n",
        "def change_slider_value(diff):\n",
        "    next_value = datapoint_slider.value + diff\n",
        "    if next_value > datapoint_slider.min and next_value < datapoint_slider.max:\n",
        "        datapoint_slider.value = next_value\n",
        "\n",
        "def incrementSliderValue(_ignore): change_slider_value(1)\n",
        "def decrementSliderValue(_ignore): change_slider_value(-1)\n",
        "\n",
        "prev_button.on_click(decrementSliderValue)\n",
        "next_button.on_click(incrementSliderValue)\n",
        "\n",
        "trigger_refresh_btn = widgets.ToggleButton(\n",
        "    value=True,\n",
        "    description=\"Randomize\",\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "def handle_widgets_update(datapoint_index: int, _randomize):\n",
        "    sample_datapoint = training_data[datapoint_index]\n",
        "    sample_datapoint = {\n",
        "        \"image\": cv2.resize(np.array(sample_datapoint[\"image\"]), (256, 256)),\n",
        "        \"label\": cv2.resize(np.array(sample_datapoint[\"label\"]), (256, 256)),\n",
        "    }\n",
        "\n",
        "    points = generate_input_points(num_positive=5, mask=sample_datapoint[\"label\"], dynamic_distance=True, logging=LoggingMode.VERBOSE)\n",
        "    bbox = get_input_bbox(sample_datapoint[\"label\"], perturbation=20)\n",
        "\n",
        "    min_distance = determine_dynamic_distance(sample_datapoint[\"label\"], logging=LoggingMode.VERBOSE) # We only need to bother with this for the visualization\n",
        "    preview_datapoint(sample_datapoint, points=points, bbox=bbox, mask=True, min_distance=min_distance)\n",
        "\n",
        "stepper_buttons = widgets.HBox([prev_button, next_button])\n",
        "interactive_plot = widgets.interactive(\n",
        "   handle_widgets_update,\n",
        "   datapoint_index=datapoint_slider,\n",
        "   _randomize=trigger_refresh_btn,\n",
        ")\n",
        "display(stepper_buttons, interactive_plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GNP1LFRozQuz"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SAMDataset(Dataset):\n",
        "  def __init__(self, dataset, processor):\n",
        "    self.dataset = dataset\n",
        "    self.processor = processor\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    datapoint = self.dataset[idx]\n",
        "    input_image = cv2.resize(np.array(datapoint[\"image\"]), (256, 256))\n",
        "    ground_truth_mask = cv2.resize(np.array(datapoint[\"label\"]), (256, 256))\n",
        "\n",
        "    # Get control points prompt.\n",
        "    input_points, input_labels = generate_input_points(\n",
        "      num_positive=5,\n",
        "      mask=ground_truth_mask,\n",
        "      dynamic_distance=True,\n",
        "      logging=LoggingMode.QUIET\n",
        "    )\n",
        "    input_points = input_points.astype(float).tolist()\n",
        "    input_labels = input_labels.tolist()\n",
        "    input_labels = [[x] for x in input_labels]\n",
        "\n",
        "    # Prepare the image and prompt for the model.\n",
        "    inputs = self.processor(\n",
        "      input_image,\n",
        "      input_points=input_points,\n",
        "      input_labels=input_labels,\n",
        "      return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Remove batch dimension which the processor adds by default.\n",
        "    inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
        "    # inputs[\"input_points\"] = inputs[\"input_points\"].squeeze(1)\n",
        "    inputs[\"input_labels\"] = inputs[\"input_labels\"].squeeze(1)\n",
        "\n",
        "    # Add ground truth segmentation.\n",
        "    inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
        "\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IttseR9AzQu0",
        "outputId": "f126aeca-b889-4ea0-aba6-6cf131c1a494"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded facebook/sam-vit-base model processor\n"
          ]
        }
      ],
      "source": [
        "from transformers import SamProcessor\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "model_size = \"base\"\n",
        "# model_size = \"huge\"\n",
        "model_name = f\"facebook/sam-vit-{model_size}\"\n",
        "\n",
        "processor = SamProcessor.from_pretrained(model_name)\n",
        "print(f\"Loaded {model_name} model processor\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7iIN29qVzQu0"
      },
      "outputs": [],
      "source": [
        "batch_size = 5\n",
        "train_dataset = SAMDataset(dataset=training_data, processor=processor)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "validation_dataset = SAMDataset(dataset=validation_data, processor=processor)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FMAaXR_zQu0",
        "outputId": "d3108edc-e0c3-48c2-c3b0-1c36adfebe36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pixel_values torch.Size([3, 1024, 1024])\n",
            "original_sizes torch.Size([2])\n",
            "reshaped_input_sizes torch.Size([2])\n",
            "input_points torch.Size([5, 1, 2])\n",
            "input_labels torch.Size([5, 1])\n",
            "ground_truth_mask (256, 256)\n"
          ]
        }
      ],
      "source": [
        "example = train_dataset[0]\n",
        "for k,v in example.items():\n",
        "  print(k,v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od8UdFAazQu0",
        "outputId": "fb9af6e7-af0a-4ecf-b620-930681289c45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pixel_values torch.Size([5, 3, 1024, 1024])\n",
            "original_sizes torch.Size([5, 2])\n",
            "reshaped_input_sizes torch.Size([5, 2])\n",
            "input_points torch.Size([5, 5, 1, 2])\n",
            "input_labels torch.Size([5, 5, 1])\n",
            "ground_truth_mask torch.Size([5, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "for k,v in batch.items():\n",
        "  print(k,v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiDyjde_zQu1",
        "outputId": "d3029f1e-1cbc-4da5-f5b5-f81bf702fcbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded facebook/sam-vit-base model, using AdamW optimizer\n"
          ]
        }
      ],
      "source": [
        "from transformers import SamModel\n",
        "from torch.optim import AdamW, Optimizer\n",
        "from typing import Any\n",
        "import monai\n",
        "\n",
        "model: Any = SamModel.from_pretrained(model_name) # type: ignore\n",
        "\n",
        "# Note: Hyperparameter tuning could improve performance here\n",
        "learning_rate = 7e-6 # 1e-5\n",
        "weight_decay = 1e-5\n",
        "optimizer_name = \"AdamW\" # Make sure to change this if changing optimizer!\n",
        "\n",
        "optimizer = AdamW(model.mask_decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "lossFn = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction=\"mean\")\n",
        "\n",
        "print(f\"Loaded {model_name} model, using {optimizer_name} optimizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gw3ZFxatzQu1"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
        "    param.requires_grad_(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzbQbYuC09-Z",
        "outputId": "02f8a43e-81d6-49d9-ec09-4150fdcfd623"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Save checkpoint path: ./checkpoints/base_AdamW_lr=7e-06_wd=1e-05_bs=5.pth\n",
            "Loading from checkpoint? False\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A subdirectory or file checkpoints already exists.\n"
          ]
        }
      ],
      "source": [
        "!mkdir checkpoints\n",
        "\n",
        "hyperparams = f\"_lr={learning_rate}_wd={weight_decay}_bs={batch_size}\"\n",
        "checkpoint_path = f\"./checkpoints/{model_size}_{optimizer_name}\" + hyperparams + \".pth\"\n",
        "load_checkpoint = False\n",
        "\n",
        "print(\"Save checkpoint path:\", checkpoint_path)\n",
        "print(\"Loading from checkpoint?\", load_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbJDvGKX1xyX"
      },
      "source": [
        "If uploading a `.pth` checkpoint to load from to Google Colab, make sure to wait until it is done uploading before continuing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5g3IwyJU4pCy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "num_epochs = 200\n",
        "save_every = 50\n",
        "\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "checkpoint = None\n",
        "if load_checkpoint:\n",
        "  checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "  model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "  optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "  print(f\"Model checkpoint loaded from: {checkpoint_path}\")\n",
        "  print(f\"Resuming training from epoch: {checkpoint['epoch']} of {num_epochs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xD0rktREzQu1"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from statistics import mean\n",
        "\n",
        "def save_model_checkpoint(model: SamModel, optimizer: Optimizer, epoch: int, best_loss: float, train_losses, validation_losses):\n",
        "  print(f\"Saving checkpoint for epoch {epoch}...\")\n",
        "  checkpoint = {\n",
        "    \"epoch\": epoch,\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    \"best_loss\": best_loss,\n",
        "    \"train_losses\": train_losses,\n",
        "    \"validation_losses\": validation_losses,\n",
        "  }\n",
        "  torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "def train_model(model: SamModel, dataloader: DataLoader, checkpoint = None, eval_every = 5):\n",
        "  model.train()\n",
        "\n",
        "  epoch = 0\n",
        "  best_loss = float(\"inf\")\n",
        "  train_losses = []\n",
        "  validation_losses = []\n",
        "\n",
        "  if checkpoint:\n",
        "    epoch = checkpoint[\"epoch\"] + 1 # Start from the next epoch\n",
        "    best_loss = checkpoint[\"best_loss\"]\n",
        "    train_losses = checkpoint[\"train_losses\"]\n",
        "    validation_losses = checkpoint[\"validation_losses\"]\n",
        "    print(f\"Resuming from checkpoint at epoch {checkpoint['epoch']}\")\n",
        "\n",
        "  while epoch < num_epochs:\n",
        "    epoch_losses = []\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Train Batch\"):\n",
        "      # Forward Pass\n",
        "      outputs = model(\n",
        "        pixel_values=batch[\"pixel_values\"].to(device),\n",
        "        input_points=batch[\"input_points\"].to(device),\n",
        "        input_labels=batch[\"input_labels\"].to(device),\n",
        "        multimask_output=False,\n",
        "      )\n",
        "\n",
        "      # Compute Loss\n",
        "      predicted_masks = outputs.pred_masks.squeeze((1, 2))\n",
        "      ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
        "      loss = lossFn(predicted_masks[:, 0], ground_truth_masks)\n",
        "\n",
        "      # Backward Pass (compute gradients of parameters w.r.t. loss)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      # Optimize Step\n",
        "      optimizer.step()\n",
        "      epoch_losses.append(loss.item())\n",
        "    # END FOR\n",
        "\n",
        "    if epoch % eval_every == 0:\n",
        "      # TODO: train_dataloader is passed as a function arg, should validation_dataloader be passed too?\n",
        "      validation_loss = evaluate_model(model, validation_dataloader)\n",
        "      validation_losses.append(validation_loss.item())\n",
        "\n",
        "    mean_loss = mean(epoch_losses)\n",
        "    if epoch % save_every == 0 or epoch == num_epochs - 1:\n",
        "      if mean_loss < best_loss:\n",
        "        save_model_checkpoint(model, optimizer, epoch, mean_loss, train_losses, validation_losses)\n",
        "        best_loss = mean_loss # TODO: Should we evaluate on validation every epoch, then save based on best validation loss?\n",
        "      else:\n",
        "        print(f\"Skipping checkpoint save since prev checkpoint is better: {best_loss} < {mean_loss}\")\n",
        "    # END IF\n",
        "\n",
        "    print(f\"Epoch: {epoch}/{num_epochs}\")\n",
        "    print(f\"Mean loss: {mean_loss}\")\n",
        "    train_losses.append(mean_loss)\n",
        "    epoch += 1\n",
        "  # END WHILE\n",
        "\n",
        "  return train_losses, validation_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "T62OhJ_BX3Jc"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "def evaluate_model(model: SamModel, dataloader: DataLoader):\n",
        "  # model.eval()\n",
        "  losses = []\n",
        "\n",
        "  for batch in tqdm(dataloader, desc=\"Eval\"):\n",
        "    # Forward Pass\n",
        "    with torch.no_grad():\n",
        "      outputs = model(\n",
        "        pixel_values=batch[\"pixel_values\"].to(device),\n",
        "        input_points=batch[\"input_points\"].to(device),\n",
        "        input_labels=batch[\"input_labels\"].to(device),\n",
        "        multimask_output=False,\n",
        "      ) # type: ignore\n",
        "\n",
        "    # Compute Loss\n",
        "    predicted_masks = outputs.pred_masks.squeeze((1, 2))\n",
        "    ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
        "    loss = lossFn(predicted_masks[:, 0], ground_truth_masks)\n",
        "    losses.append(loss)\n",
        "\n",
        "    # Avoid Memory Overload\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "  # END FOR\n",
        "\n",
        "  return sum(losses) / len(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "o8kEMz-22E_t"
      },
      "outputs": [],
      "source": [
        "# Evaluate baseline train data performance:\n",
        "# print(\"Average train loss:\", evaluate_model(model, train_dataloader))\n",
        "\n",
        "# Evaluate zero-shot model performance:\n",
        "# print(\"\\nAverage validation loss:\", evaluate_model(model, validation_dataloader))\n",
        "\n",
        "# TODO: Probably include the test set here in the future too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOEtxzZLrcYJ"
      },
      "source": [
        "TODO: Evaluate model on the validation dataset every N rounds, for plotting purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oqcJcXAcEzd",
        "outputId": "32e84155-decc-488b-c7a3-49fbb8b27bd4"
      },
      "outputs": [],
      "source": [
        "eval_every = 5\n",
        "train_losses, validation_losses = train_model(model, train_dataloader, checkpoint, eval_every=eval_every)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uL768el_XpPY"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# files.download(checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRfqRCxaNRWK"
      },
      "outputs": [],
      "source": [
        "val_loss_epochs = [i * eval_every for i in range(len(validation_losses))]\n",
        "\n",
        "plt.plot(train_losses, label=\"Train\")\n",
        "plt.plot(val_loss_epochs, validation_losses, label=\"Validation\")\n",
        "plt.title(f\"Epoch vs Loss (model={model_size}, optim={optimizer_name}, lr={learning_rate}, wd={weight_decay})\")\n",
        "plt.xlabel(\"Training Epoch #\")\n",
        "plt.ylabel(\"Mean Epoch Train Loss\")\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-XaLX6uJiLV"
      },
      "source": [
        "| Model        | Optimizer | Learning Rate   | Weight Decay | Batch Size | Epochs | Prompts | Train Loss | Validation Loss |\n",
        "|--------------|-----------|-----------------|--------------|------------|--------|---------|------------|-----------------|\n",
        "| sam-vit-base | AdamW     | 1e-5            | 0            | 2          | 300    | points  | 220.9510   | 235.2056        |\n",
        "| sam-vit-base | AdamW     | 3e-5            | 0            | 3          | 300    | points  | 220.3394   | 245.4707        |\n",
        "| sam-vit-base | AdamW     | 7e-6            | 0            | 3          | 600    | points  | 220.4751   | 223.8311        |\n",
        "| sam-vit-base | AdamW     | 7e-6            | 1e-4         | 4          | 300    | points  | 222.0167   | 219.9485        |\n",
        "| sam-vit-huge | AdamW     | 1e-5            | 0            | 2          | 200    | points  | 221.4048   | 255.4305        |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsEWMgKdJPnY"
      },
      "outputs": [],
      "source": [
        "# Evaluate performance after training:\n",
        "print(\"Average train loss:\", evaluate_model(model, train_dataloader))\n",
        "print(\"\\nAverage validation loss:\", evaluate_model(model, validation_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbknISjxAtkF"
      },
      "outputs": [],
      "source": [
        "def visually_evaluate_model(model: SamModel, dataloader: DataLoader, displayCount: int = float(\"inf\")):\n",
        "  model.eval()\n",
        "  iterCount = 0\n",
        "\n",
        "  for batch in tqdm(dataloader, desc=\"Eval\"):\n",
        "    # Forward Pass\n",
        "    with torch.no_grad():\n",
        "      outputs = model(\n",
        "        pixel_values=batch[\"pixel_values\"].to(device),\n",
        "        input_points=batch[\"input_points\"].to(device),\n",
        "        input_labels=batch[\"input_labels\"].to(device),\n",
        "        multimask_output=False,\n",
        "      ) # type: ignore\n",
        "    predicted_masks = torch.sigmoid(outputs.pred_masks.squeeze((1, 2)))\n",
        "\n",
        "    # ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
        "    # loss = lossFn(predicted_masks[:, 0], ground_truth_masks)\n",
        "\n",
        "    for batch_item in range(predicted_masks.shape[0]):\n",
        "      fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,10))\n",
        "      raw_mask = predicted_masks[batch_item, 0].cpu().numpy()\n",
        "      mask = (raw_mask > 0.5).astype(np.uint8) # TODO: Replace 0.5 with variable confidence threshold.\n",
        "\n",
        "      overlay_mask(mask, axes[0])\n",
        "      axes[1].imshow(batch[\"ground_truth_mask\"][batch_item])\n",
        "\n",
        "      input_points = batch[\"input_points\"][batch_item] / 4\n",
        "      show_control_points(input_points, batch[\"input_labels\"][batch_item], axes[0])\n",
        "      show_control_points(input_points, batch[\"input_labels\"][batch_item], axes[1])\n",
        "\n",
        "    if iterCount >= displayCount:\n",
        "      break\n",
        "    iterCount += 1\n",
        "  # END FOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uj0VDvn2iigi"
      },
      "outputs": [],
      "source": [
        "visually_evaluate_model(model, validation_dataloader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "475a08db99efb31e89a9087e9ca2d38d56797a16dd41a0621d20f3dc95701503"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
